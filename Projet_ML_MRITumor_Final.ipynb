{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csrnUdCioNET"
      },
      "source": [
        "# Machine Learning Project - MRI Brain Tumor Dataset\n",
        "Mouna Ahamdy et Roc de Larouzière"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrCVNri0QcKj"
      },
      "outputs": [],
      "source": [
        "#All necessary imports :\n",
        "\n",
        "#Import des bibliothèques de base\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#Traitement et analyse d'images\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "#Import de Tensorflow et Keras\n",
        "import tensorflow\n",
        "from keras import datasets, layers, models\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, GlobalAveragePooling2D\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import VGG16, ResNet50\n",
        "from keras.applications.vgg16 import preprocess_input as preprocess_inputvgg16\n",
        "from keras.applications.resnet50 import preprocess_input as preprocess_inputresnet50\n",
        "\n",
        "\n",
        "\n",
        "#Import de Scikit Learn\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "\n",
        "TAILLE_IMAGE = 224\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHBrBItwqhIT"
      },
      "source": [
        "## STEP 1 : Loading the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adCb-lpcy9mm"
      },
      "outputs": [],
      "source": [
        "#Loading data thanks to Kaggle API keys -- Mouna load\n",
        "! pip install -q kaggle\n",
        "! mkdir ~/.kaggle/\n",
        "! cp \"/content/drive/MyDrive/kaggle.json\" ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset\n",
        "! unzip /content/brain-tumor-mri-dataset.zip -d archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qc59VNK-6nN"
      },
      "outputs": [],
      "source": [
        "#Directly loading from GoogleDrive after importing the Dataset on it -- Roc load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9lJNHnXuSft"
      },
      "source": [
        "## Step 2 : Visualisation of the dataset and getting the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoZXZMxNxOoS"
      },
      "outputs": [],
      "source": [
        "#Training and testing directories\n",
        "\n",
        "#Mouna\n",
        "train_dir = \"/content/archive/Training/\"\n",
        "test_dir = \"/content/archive/Testing/\"\n",
        "\n",
        "#Roc\n",
        "#train_dir = \"/content/drive/My Drive/archive/Training/\"\n",
        "#test_dir = \"/content/drive/My Drive/archive/Testing/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKPLX9yJcXZM"
      },
      "outputs": [],
      "source": [
        "#Definition des chemins pour accéder aux sous dossiers du dataset\n",
        "train_subdirs = [os.path.join(train_dir, subdir) for subdir in os.listdir(train_dir)]\n",
        "test_subdirs = [os.path.join(test_dir, subdir) for subdir in os.listdir(test_dir)]\n",
        "\n",
        "#Compte du nombre d'images dans chaque sous dossiers\n",
        "num_train_samples = sum([len(files) for subdir in train_subdirs for r, d, files in os.walk(subdir)])\n",
        "num_test_samples = sum([len(files) for subdir in test_subdirs for r, d, files in os.walk(subdir)])\n",
        "\n",
        "\n",
        "#Recuperation des labels\n",
        "def SetLabels(dir) :\n",
        "    labelsCorrespondance = set()\n",
        "    labels = []\n",
        "    listeTumeur = os.listdir(dir)\n",
        "    for ssdossIndex, ssdossNom in enumerate(listeTumeur):\n",
        "        ssdossChemin = os.path.join(dir, ssdossNom)\n",
        "        nbimages_sousdossier = len([img for img in os.listdir(ssdossChemin)])\n",
        "        labelsCorrespondance.add((ssdossIndex, ssdossNom, nbimages_sousdossier)) #ssdossIndex est l'index du dossier dans listeTumeur et ssdossNom est le nom de la tumeur (titre du sous dossier)\n",
        "        labels.extend([ssdossIndex] * nbimages_sousdossier)\n",
        "    return labels, labelsCorrespondance\n",
        "\n",
        "\n",
        "y_train, y_trainCorrespondance = SetLabels(train_dir)\n",
        "y_test, y_testCorrespondance = SetLabels(test_dir)\n",
        "\n",
        "\n",
        "#Pie chart pour la visualisation des données\n",
        "def PieChart(labelsCorrespondance, num_samples, title) :\n",
        "  labels = [elem[1] for elem in labelsCorrespondance]\n",
        "  sizes = [elem[2] / num_samples for elem in labelsCorrespondance]\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
        "  ax.axis('equal')\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#Print des informations du dataset\n",
        "print('DATASET TRAIN PART : ')\n",
        "for elem in y_trainCorrespondance :\n",
        "  print(\"Il y a \", elem[2], \" exemples de tumeur de type \", elem[1], \" pour un ensemble de \", num_train_samples, \" données train\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print('DATASET TEST PART : ')\n",
        "for elem in y_testCorrespondance :\n",
        "  print(\"Il y a \", elem[2], \" exemples de tumeur de type \", elem[1], \" pour un ensemble de \", num_test_samples, \" données test\")\n",
        "\n",
        "print(\"\\n\\n\\n\\n\\n\\n\")\n",
        "PieChart(y_trainCorrespondance, num_train_samples, \"Repartition des données dans la partie Train du Dataset\")\n",
        "PieChart(y_testCorrespondance, num_test_samples, \"Repartition des données dans la partie Test du Dataset\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCMxWRJ28eYE"
      },
      "source": [
        "## Step 3 : Embedding de notre DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8Amgq8mwMhx"
      },
      "outputs": [],
      "source": [
        "#Modèle vgg16 pour l'embedding de nos données X_train et X_test du dataset\n",
        "def vgg16model() :\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = Flatten()(base_model.output)\n",
        "    x = Dense(1024, activation='relu')(x) # Ajoute une couche Dense avec 1024 unités et activation ReLU pour passer de 50 000 unités à 1024\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=x)  # Crée un nouveau modèle qui inclut les modifications\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "#Modèle ResNet pour l'embedding de nos données X_train et X_test du dataset\n",
        "def resnetmodel() :\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Ajout de GlobalAveragePooling pour réduire la dimensionalité\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=x)   # Crée un nouveau modèle qui inclut les modifications\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qiq4ao6BaMI7"
      },
      "outputs": [],
      "source": [
        "#La fonction extract features va faire un pre traitement des images du dataset grâce à la fonction ImageDataGenerator de Keras\n",
        "#Pour vgg16, le paramètre modelsizeoutput = 1024 et pour ResNet, modelsizeoutput = 2048\n",
        "\n",
        "def extract_features(directory, sample_count, modelsizeoutput, model, preprocess_input):\n",
        "    model = model\n",
        "    features = np.zeros((sample_count, modelsizeoutput))  # modelsizeoutput ajuste la taille pour correspondre à la sortie de la couche Dense\n",
        "    generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "    image_data = generator.flow_from_directory(\n",
        "        directory,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode=None,\n",
        "        shuffle=False\n",
        "    )\n",
        "    i = 0\n",
        "    for inputs_batch in image_data:\n",
        "        features_batch = model.predict(inputs_batch, verbose=1)\n",
        "        features[i * 32 : (i + 1) * 32] = features_batch\n",
        "        print(f\"Traitement du lot {i+1}, {len(inputs_batch)} images traitées.\")\n",
        "        i += 1\n",
        "        if i * 32 >= sample_count:\n",
        "            break\n",
        "    return features\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiN6WRjR16JT"
      },
      "outputs": [],
      "source": [
        "# Extract features\n",
        "\n",
        "#si le model utilisé est vgg16:\n",
        "#train_features = extract_features(train_dir, num_train_samples, 1024, vgg16model(), preprocess_inputvgg16)\n",
        "#test_features = extract_features(test_dir, num_test_samples, 1024, vgg16model(), preprocess_inputvgg16)\n",
        "\n",
        "#si le model utilisé est ResNet50 :\n",
        "train_features = extract_features(train_dir, num_train_samples, 2048, resnetmodel(), preprocess_inputresnet50)\n",
        "test_features = extract_features(test_dir, num_test_samples, 2048, resnetmodel(), preprocess_inputresnet50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2QWE9Ue2pph"
      },
      "outputs": [],
      "source": [
        "#PCA\n",
        "\n",
        "pca = PCA(n_components=256, random_state=42)\n",
        "train_features_pca = pca.fit_transform(train_features)\n",
        "test_features_pca = pca.transform(test_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YceG2t4-KOr-"
      },
      "source": [
        "## Step 4 : Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4j9Z8Ek8iAd"
      },
      "outputs": [],
      "source": [
        "#Logistic Regression\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "model = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=10000)  # multi_class='ovr' pour one-vs-all\n",
        "scores = cross_val_score(model, train_features, y_train, cv=skf, scoring='accuracy', verbose=10)\n",
        "print(\"Accuracy de chaque fold:\", scores)\n",
        "print(\"Accuracy moyenne:\", np.mean(scores))\n",
        "\n",
        "# Entraîner le modèle sur l'ensemble d'apprentissage complet\n",
        "model.fit(train_features_pca, y_train)\n",
        "\n",
        "# Prédire les étiquettes pour l'ensemble de test\n",
        "y_pred = model.predict(test_features_pca)\n",
        "\n",
        "# Calculer la précision sur l'ensemble de test\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy sur l'ensemble de test:\", test_accuracy)\n",
        "lg_accuracy = test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wxx-bXLQMgLs"
      },
      "outputs": [],
      "source": [
        "#SVM\n",
        "\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "param_grid = {'C': [0.01, 1, 5, 10, 25, 50, 80, 100]}  # Exemple de valeurs pour C\n",
        "\n",
        "model = LinearSVC(dual=False, multi_class='ovr', max_iter=5000)\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, scoring='accuracy', cv=skf, verbose=10)\n",
        "grid_search.fit(train_features_pca, y_train)\n",
        "\n",
        "print(\"Meilleur paramètre (C):\", grid_search.best_params_)\n",
        "print(\"Meilleure accuracy de cross-validation:\", grid_search.best_score_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(test_features_pca)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy sur l'ensemble de test:\", test_accuracy)\n",
        "\n",
        "lsvm_accuracy = test_accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCKLjZBrjARw"
      },
      "outputs": [],
      "source": [
        "#SVM non lineaire\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "param_grid = {'C': [6.8, 7, 8, 9], #valeurs pour resnet50, 1.19 correspondant à la meilleure accuracy\n",
        "    'gamma': ['scale', 'auto']  # Exemple de valeurs pour gamma\n",
        "}\n",
        "\n",
        "model = SVC(kernel='rbf', max_iter=10000)  # Utilisation du noyau RBF\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, scoring='accuracy', cv=skf, verbose=10)\n",
        "grid_search.fit(train_features_pca, y_train)\n",
        "\n",
        "print(\"Meilleur paramètre (C, gamma):\", grid_search.best_params_)\n",
        "print(\"Meilleure accuracy de cross-validation:\", grid_search.best_score_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(test_features_pca)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy sur l'ensemble de test:\", test_accuracy)\n",
        "nlsvm_accuracy = test_accuracy"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}